# ExecPrompt â€” Multi-Provider Endpoint Enhancement

**Date:** 2026-02-13 (Friday the 13th)  
**Branch:** `feature/multi-provider-endpoints`  
**Base:** `main` (clean, all previous phases complete)  
**Status:** IMPLEMENTATION COMPLETE (Phases 8Aâ€“8E)

---

## Table of Contents

1. [Vision Statement](#1-vision-statement)
2. [The Problem We're Solving](#2-the-problem-were-solving)
3. [OpenAI-Compatible API Spec Deep Dive](#3-openai-compatible-api-spec-deep-dive)
4. [Provider Analysis: OpenAI, Anthropic, Gemini, OpenRouter](#4-provider-analysis-openai-anthropic-gemini-openrouter)
5. [Architecture Design: Endpoint Management System](#5-architecture-design-endpoint-management-system)
6. [UI/UX Design: Model Selector & Endpoint Configuration](#6-uiux-design-model-selector--endpoint-configuration)
7. [Web Search Tool Compatibility Analysis](#7-web-search-tool-compatibility-analysis)
8. [Future Tool Proposals](#8-future-tool-proposals)
9. [Pre-Implementation Code Analysis](#9-pre-implementation-code-analysis)
10. [Gap Analysis & Risk Assessment](#10-gap-analysis--risk-assessment)
11. [Implementation Plan](#11-implementation-plan)

---

## 1. Vision Statement

ExecPrompt is already the definitive Ollama client â€” a meticulously crafted terminal-aesthetic interface that delivers a premium, intentional experience. The current single-endpoint architecture assumes one Ollama server. Phase 8 evolves ExecPrompt from a **single-provider Ollama client** into a **multi-provider AI command center** while preserving every pixel of the experience that makes it exceptional.

### The Full Vision

A user opens ExecPrompt. In the settings, they configure **named endpoints** â€” each with a custom label, URL, optional API key, and a curated selection of models:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–¸ ENDPOINTS                                 â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â— Ollama Cloud        ollama.com      3 models â”‚
â”‚ â—‹ Ollama Private      10.0.0.50:11434 5 models â”‚
â”‚ â—‹ OpenRouter          openrouter.ai   6 models â”‚
â”‚                                             â”‚
â”‚ [+ ADD ENDPOINT]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

When the user taps the model selector in the chat app bar, they see a **grouped, searchable** list of only their curated models, organized by endpoint:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–¸ SELECT MODEL               â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ ğŸ” Search models...          â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â”€â”€ OLLAMA CLOUD â”€â”€           â”‚
â”‚ â—‹ qwen2.5:14b         9.0G  â”‚
â”‚ â— deepseek-r1:14b     9.0G  â”‚
â”‚ â—‹ gemma3:12b           8.1G  â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â”€â”€ OLLAMA PRIVATE â”€â”€         â”‚
â”‚ â—‹ codellama:34b       19.0G  â”‚
â”‚ â—‹ mistral:7b           4.1G  â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â”€â”€ OPENROUTER â”€â”€             â”‚
â”‚ â—‹ claude-3.5-sonnet          â”‚
â”‚ â—‹ gpt-4o                     â”‚
â”‚ â—‹ gemini-2.0-flash           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Selecting any model automatically routes the request to the correct endpoint â€” the user just picks a model and chats. The experience is identical to today, except the model picker is now a unified command center across all configured providers.

### Guiding Principles

1. **Zero regression** â€” Existing Ollama-only users should notice nothing different (unless they explore settings)
2. **Brand-coherent** â€” Every new UI element matches the CyberTerm terminal aesthetic
3. **Progressive disclosure** â€” Simple by default, powerful when configured
4. **Hot-swappable** â€” Switch providers mid-session by selecting a different model
5. **API keys stay secure** â€” Continue using `flutter_secure_storage` for all secrets

---

## 2. The Problem We're Solving

### Current Architecture (Single-Provider)

```
Settings:
  baseUrl = "https://ollama.com"
  apiKey = "sk-xxx"
  
Flow:
  User â†’ selects model â†’ OllamaApiService(baseUrl) â†’ /api/chat â†’ stream response
```

**Limitations:**
- Only one endpoint at a time
- Changing providers requires manually editing URL + API key in Settings
- Model list comes from one source (Ollama's `/api/tags`)
- All API calls assume Ollama-native endpoints (`/api/chat`, `/api/tags`)
- No way to save/recall endpoint configurations

### Target Architecture (Multi-Provider)

```
Endpoints (saved, named):
  "Ollama Cloud"   â†’ baseUrl: ollama.com,     apiKey: sk-xxx,    type: ollama
  "Ollama Private"  â†’ baseUrl: 10.0.0.50:11434, apiKey: null,    type: ollama
  "OpenRouter"      â†’ baseUrl: openrouter.ai,   apiKey: sk-or-xx, type: openai

Flow:
  User â†’ selects model â†’ resolve endpoint â†’ ApiService(endpoint) â†’ correct API path â†’ stream response
```

---

## 3. OpenAI-Compatible API Spec Deep Dive

### 3.1 The OpenAI Chat Completions Standard

The OpenAI Chat Completions API (`POST /v1/chat/completions`) is the de facto standard that nearly every major provider implements. Key characteristics:

**Request Format:**
```json
{
  "model": "gpt-4o",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi there!"},
    {"role": "user", "content": "What is 2+2?"}
  ],
  "stream": true,
  "temperature": 0.7,
  "top_p": 0.9,
  "max_tokens": 4096,
  "tools": [...],
  "tool_choice": "auto"
}
```

**Streaming Response (SSE - Server-Sent Events):**
```
data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","model":"gpt-4o","choices":[{"index":0,"delta":{"role":"assistant","content":"The"},"finish_reason":null}]}

data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","model":"gpt-4o","choices":[{"index":0,"delta":{"content":" answer"},"finish_reason":null}]}

data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","model":"gpt-4o","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

**Key Differences from Ollama Native API:**

| Aspect | Ollama Native (`/api/chat`) | OpenAI Spec (`/v1/chat/completions`) |
|--------|-------------------------|--------------------------------------|
| **Endpoint** | `/api/chat` | `/v1/chat/completions` |
| **Streaming** | NDJSON (newline-delimited) | SSE (`data: {...}`) |
| **Done signal** | `{"done": true}` in final JSON | `"finish_reason": "stop"` + `data: [DONE]` |
| **Content** | `message.content` | `choices[0].delta.content` (streaming) |
| **Tool calls** | `message.tool_calls[].function` | `choices[0].delta.tool_calls[].function` |
| **Thinking** | `message.thinking` (Ollama extension) | Varies by provider (see below) |
| **Model list** | `GET /api/tags` â†’ `{models: [...]}` | `GET /v1/models` â†’ `{data: [...]}` |
| **Model info** | `size`, `details.parameterSize`, `quantizationLevel` | `id`, `created`, `owned_by` (minimal) |
| **Options** | `options: {temperature, top_k, ...}` | Top-level: `temperature`, `top_p`, `max_tokens` |
| **Keep alive** | `keep_alive: "5m"` | Not applicable |
| **Images** | `images: ["base64..."]` in message | `content: [{type: "image_url", image_url: {url: "data:image/..."}}]` |
| **Auth** | `Authorization: Bearer <key>` | `Authorization: Bearer <key>` |

### 3.2 Ollama's OpenAI-Compatible Mode

**Critical insight:** Ollama itself provides an OpenAI-compatible endpoint at `/v1/chat/completions` and `/v1/models`. This means we could potentially use a single OpenAI-spec service for ALL providers including Ollama, but with some caveats:

- Ollama's OpenAI-compatible mode does support tool calls
- Ollama's OpenAI-compatible mode does **not** return `thinking` content
- Ollama's native API returns richer model metadata (size, quantization, parameter count)
- Ollama's native API supports `keep_alive` for memory management

**Decision:** We keep the native Ollama API service for Ollama-type endpoints (to preserve thinking, model metadata, keep_alive) and use a new OpenAI-compatible service for non-Ollama endpoints.

### 3.3 Tool Calls in OpenAI Spec

Tool/function calling follows this pattern across all OpenAI-spec providers:

**Request:**
```json
{
  "tools": [{
    "type": "function",
    "function": {
      "name": "web_search",
      "description": "Search the web",
      "parameters": {
        "type": "object",
        "properties": {
          "query": {"type": "string", "description": "Search query"}
        },
        "required": ["query"]
      }
    }
  }],
  "tool_choice": "auto"
}
```

**Response (streaming, tool call):**
```
data: {"choices":[{"delta":{"tool_calls":[{"index":0,"id":"call_xxx","type":"function","function":{"name":"web_search","arguments":""}}]}}]}
data: {"choices":[{"delta":{"tool_calls":[{"index":0,"function":{"arguments":"{\"query\""}}]}}]}
data: {"choices":[{"delta":{"tool_calls":[{"index":0,"function":{"arguments":": \"latest news\"}"}}]}}]}
data: {"choices":[{"delta":{},"finish_reason":"tool_calls"}]}
```

**Tool result submission:**
```json
{
  "messages": [
    ...previous_messages,
    {"role": "assistant", "tool_calls": [{"id": "call_xxx", "type": "function", "function": {"name": "web_search", "arguments": "{\"query\": \"latest news\"}"}}]},
    {"role": "tool", "tool_call_id": "call_xxx", "content": "[{\"title\": \"...\", ...}]"}
  ]
}
```

**Key difference from Ollama:** OpenAI spec requires `tool_call_id` for matching tool results to their calls. Ollama's native API uses `tool_name` instead.

### 3.4 Thinking/Reasoning

| Provider | Thinking Support | Mechanism |
|----------|-----------------|-----------|
| **Ollama (native)** | Yes | `message.thinking` field in response |
| **Ollama (OpenAI mode)** | No | Not exposed in OpenAI-compatible mode |
| **OpenAI** | Yes (o1, o3 models) | `reasoning_content` in response |
| **Anthropic (via OpenAI spec)** | Varies | `thinking` blocks in content array or provider-specific |
| **OpenRouter** | Yes | Passes through provider's thinking format |
| **Gemini** | Yes (2.0 Flash Thinking) | `thought` field or equivalent |

**Strategy:** We'll normalize thinking content in our response adapter. For Ollama native, use `message.thinking`. For OpenAI-spec providers, check for `reasoning_content` or `thinking` fields. Display uniformly in the existing collapsible thinking section.

### 3.5 Image/Vision Support

| Provider | Image Format |
|----------|-------------|
| **Ollama (native)** | `images: ["base64"]` array in message |
| **OpenAI spec** | `content: [{type: "image_url", image_url: {url: "data:image/jpeg;base64,..."}}]` |

**Strategy:** Our `ChatMessage` model already handles base64 images. The API service adapter will convert between formats based on endpoint type.

---

## 4. Provider Analysis: OpenAI, Anthropic, Gemini, OpenRouter

### 4.1 OpenAI

| Aspect | Details |
|--------|---------|
| **Base URL** | `https://api.openai.com` |
| **Chat endpoint** | `POST /v1/chat/completions` |
| **Models endpoint** | `GET /v1/models` |
| **Auth** | `Authorization: Bearer sk-...` |
| **Streaming** | SSE format |
| **Tool calling** | Full support with `tool_call_id` |
| **Thinking** | `reasoning_content` on o1/o3 models |
| **Vision** | `image_url` content blocks |
| **Model list size** | ~20-30 models (plus fine-tunes) |

### 4.2 Anthropic (via OpenAI-compatible endpoint)

Anthropic offers an OpenAI-compatible endpoint at `https://api.anthropic.com`:

| Aspect | Details |
|--------|---------|
| **Base URL** | `https://api.anthropic.com` |
| **Chat endpoint** | `POST /v1/messages` (native) or use via OpenRouter |
| **Auth** | `x-api-key: sk-ant-...` (native) or `Authorization: Bearer` (via OpenRouter) |
| **Tool calling** | Full support |
| **Thinking** | Extended thinking via `thinking` content blocks |
| **Vision** | Base64 images in `content` array |
| **Note** | Anthropic's native API is NOT OpenAI-compatible. Best accessed via OpenRouter for uniform interface. |

**Strategy:** For Anthropic direct access, we'd need a separate adapter. For simplicity in Phase 8, recommend accessing Anthropic models through OpenRouter, which wraps them in standard OpenAI spec.

### 4.3 Google Gemini

| Aspect | Details |
|--------|---------|
| **Base URL** | `https://generativelanguage.googleapis.com` |
| **OpenAI-compat** | `https://generativelanguage.googleapis.com/v1beta/openai/` |
| **Chat endpoint** | `POST /v1beta/openai/chat/completions` |
| **Models endpoint** | `GET /v1beta/openai/models` |
| **Auth** | `Authorization: Bearer <API_KEY>` |
| **Tool calling** | Supported via OpenAI-compatible mode |
| **Thinking** | Gemini 2.0 Flash Thinking â€” exposed as `thought` |
| **Vision** | Supported |
| **Model list size** | ~10-15 models |

### 4.4 OpenRouter

| Aspect | Details |
|--------|---------|
| **Base URL** | `https://openrouter.ai/api` |
| **Chat endpoint** | `POST /v1/chat/completions` |
| **Models endpoint** | `GET /v1/models` |
| **Auth** | `Authorization: Bearer sk-or-...` |
| **Streaming** | Standard SSE |
| **Tool calling** | Supported (passes through to underlying provider) |
| **Thinking** | Passes through provider's thinking format |
| **Vision** | Supported for vision-capable models |
| **Model list size** | **100-300+ models** â€” this is the big one |
| **Extra headers** | `HTTP-Referer` and `X-Title` recommended |
| **Model info** | Rich metadata: pricing, context length, capabilities |

**OpenRouter is the swiss army knife** â€” it provides access to OpenAI, Anthropic, Google, Meta, Mistral, and dozens of other providers through a single OpenAI-compatible API. This makes it the ideal "catch-all" provider.

### 4.5 Provider-Type Classification

For our implementation, we classify endpoints into two types:

| Type | API Format | Providers | Service Used |
|------|-----------|-----------|-------------|
| **`ollama`** | Ollama native (`/api/chat`, `/api/tags`) | Ollama Cloud, Ollama self-hosted | `OllamaApiService` (existing) |
| **`openai`** | OpenAI-compatible (`/v1/chat/completions`, `/v1/models`) | OpenAI, Gemini, OpenRouter, LM Studio, etc. | `OpenAiApiService` (new) |

This two-type system covers virtually every provider while keeping the implementation clean. Users don't need to select a "type" â€” we can auto-detect based on URL patterns, or let them pick.

---

## 5. Architecture Design: Endpoint Management System

### 5.1 Data Model: `Endpoint`

```dart
/// Represents a configured API endpoint with its credentials and selected models.
class Endpoint {
  final String id;           // UUID
  final String name;         // User-given name: "Ollama Cloud", "OpenRouter", etc.
  final String baseUrl;      // e.g., "https://ollama.com", "https://openrouter.ai/api"
  final String? apiKey;      // Stored in flutter_secure_storage, referenced by endpoint ID
  final EndpointType type;   // ollama | openai
  final List<String> selectedModels;  // User's curated model names
  final int sortOrder;       // For UI ordering
  final bool isActive;       // Whether this endpoint should be used
  final DateTime createdAt;
  final DateTime updatedAt;
}

enum EndpointType { ollama, openai, anthropic }
```

> **Design note:** The enum is deliberately extensible. Adding a new provider
> (e.g., `cohere`, `mistralPlatform`) is simply a new enum value + a new
> `ApiAdapter` implementation â€” zero changes to existing adapters or the router.

### 5.2 Storage Strategy

| Data | Storage | Reason |
|------|---------|--------|
| Endpoint metadata (name, URL, type, selected models, sort order) | `SharedPreferences` (JSON-encoded list) | Simple, synchronous access, consistent with existing settings |
| API keys | `flutter_secure_storage` | Security â€” keys stored encrypted at rest |
| Model cache (fetched model lists per endpoint) | In-memory (Riverpod state) | Transient, refreshed on demand |

**Why not the DB?** Endpoints are configuration, not user content. SharedPreferences is where all other settings live. This keeps the pattern consistent and avoids a DB migration.

### 5.3 Service Layer: Pluggable Adapter Architecture

Rather than a rigid two-type system, we use an **abstract `ApiAdapter`** that
each provider implements. This is the core extensibility mechanism â€” adding a
new provider means writing one new class, not touching any existing code.

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   ChatProvider       â”‚
                    â”‚   (orchestrator)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ApiServiceRouter    â”‚
                    â”‚  resolve(endpoint)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ OllamaApi  â”‚ â”‚ OpenAiApi  â”‚ â”‚ AnthropicApi  â”‚
           â”‚ Adapter    â”‚ â”‚ Adapter    â”‚ â”‚ Adapter       â”‚
           â”‚ (existing) â”‚ â”‚ (new)      â”‚ â”‚ (new)         â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           /api/chat       /v1/chat/       /v1/messages
           /api/tags       completions     (models: curated)
           NDJSON stream   SSE stream      SSE stream
```

**`ApiAdapter`** (abstract class):
```dart
abstract class ApiAdapter {
  /// Stream a chat completion, yielding normalized response chunks.
  Stream<ChatResponse> streamChat(ChatRequest request);

  /// List available models for this endpoint.
  Future<List<ModelInfo>> listModels();

  /// Cancel any in-flight request.
  void cancelActiveRequest();

  /// Build a tool-result message in the format this provider expects.
  ChatMessage buildToolResultMessage({
    required String toolName,
    required String content,
    String? toolCallId,  // required for OpenAI/Anthropic, null for Ollama
  });

  /// Update the base URL at runtime.
  void updateBaseUrl(String url);

  /// Update the API key at runtime.
  void updateApiKey(String? key);
}
```

**`OllamaAdapter`** â€” wraps the existing `OllamaApiService` (no refactor needed).

**`OpenAiAdapter`** (new) â€” speaks OpenAI-spec:
- `streamChat(request)` â†’ `POST /v1/chat/completions` with SSE parsing
- `listModels()` â†’ `GET /v1/models` â†’ normalized model list
- Request/response adapters to translate between our internal models and OpenAI format
- Handles OpenRouter-specific headers (`HTTP-Referer`, `X-Title`)

**`AnthropicAdapter`** (new) â€” speaks Anthropic Messages API:
- `streamChat(request)` â†’ `POST /v1/messages` with SSE parsing
- `listModels()` â†’ returns curated list (Anthropic has no /models endpoint)
- System prompt sent via top-level `system` field (not as a message)
- Tool use via `tool_use` / `tool_result` content blocks
- `anthropic-version` header required

> **Why not just proxy Anthropic through OpenRouter?** We *can*, and users who
> prefer that workflow will simply add an OpenRouter endpoint. But a dedicated
> adapter gives: (1) direct billing at Anthropic's lower prices, (2) full
> feature fidelity (extended thinking, prompt caching), (3) no intermediary
> latency. The user explicitly asked us to "do it RIGHT" with dedicated
> adapters.

**`ApiServiceRouter`** â€” thin dispatcher:  
Given a model name â†’ look up which endpoint owns it â†’ `switch` on `EndpointType`
â†’ return the correct adapter instance (cached per endpoint).

### 5.4 Provider State Management

```dart
/// All configured endpoints
final endpointsProvider = StateNotifierProvider<EndpointsNotifier, List<Endpoint>>(...)

/// Active endpoint for a given model name
final endpointForModelProvider = Provider.family<Endpoint?, String>((ref, modelName) {
  final endpoints = ref.watch(endpointsProvider);
  return endpoints.firstWhereOrNull((e) => e.selectedModels.contains(modelName));
});

/// All available models across all endpoints (grouped)
final allModelsProvider = Provider<Map<String, List<ModelInfo>>>((ref) {
  // Returns: { "Ollama Cloud": [model1, model2], "OpenRouter": [model3, ...] }
});

/// Currently selected model (unchanged interface, but now resolves to any endpoint)
final selectedModelProvider = StateProvider<String?>((ref) { /* unchanged */ });
```

### 5.5 The Selected Model â†’ Endpoint Resolution

When the user selects a model, we need to know which endpoint to send the request to. The resolution is:

1. User picks "claude-3.5-sonnet" from the grouped model picker
2. We look up which `Endpoint` has "claude-3.5-sonnet" in its `selectedModels`
3. We resolve the endpoint's `EndpointType` â†’ instantiate the correct `ApiAdapter` (`OllamaAdapter`, `OpenAiAdapter`, or `AnthropicAdapter`)
4. We send the chat request through that adapter (format translation is internal to the adapter)

**Edge case:** Two endpoints could theoretically have a model with the same name (e.g., two Ollama instances both running `llama3.2`). We handle this by prefixing with endpoint name in the model ID stored internally: `endpoint_id:model_name`. The display name strips the prefix.

---

## 6. UI/UX Design: Model Selector & Endpoint Configuration

### 6.1 Endpoint Configuration Screen

Located in Settings, replaces the current "CONNECTION" section:

```
â”€â”€ ENDPOINTS â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â— Ollama Cloud                         [â‹®] â”‚
â”‚   ollama.com â€¢ 3 models selected            â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â—‹ Ollama Private                       [â‹®] â”‚
â”‚   10.0.0.50:11434 â€¢ 5 models selected      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â—‹ OpenRouter                           [â‹®] â”‚
â”‚   openrouter.ai â€¢ 6 models selected        â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                             â”‚
â”‚  [+ ADD ENDPOINT]                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Add/Edit Endpoint Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–¸ NEW ENDPOINT                   â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Name                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ My Ollama Cloud              â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚
â”‚ Endpoint Type                    â”‚
â”‚ â— Ollama  â—‹ OpenAI  â—‹ Anthropic  â”‚
â”‚                                  â”‚
â”‚ Server URL                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ https://ollama.com           â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚
â”‚ API Key (optional)               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢                 â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                  â”‚
â”‚ [TEST CONNECTION]                â”‚
â”‚ â— Connected â€” Ollama v0.5.7     â”‚
â”‚                                  â”‚
â”‚ â”€â”€ SELECT MODELS â”€â”€              â”‚
â”‚ ğŸ” Filter models...             â”‚
â”‚ â˜‘ deepseek-r1:14b        9.0G  â”‚
â”‚ â˜‘ qwen2.5:14b            9.0G  â”‚
â”‚ â˜ llama3.2:latest        2.0G  â”‚
â”‚ â˜ codellama:13b          7.4G  â”‚
â”‚ â˜‘ gemma3:12b             8.1G  â”‚
â”‚ â˜ mistral:7b             4.1G  â”‚
â”‚ ... (scrollable)                â”‚
â”‚                                  â”‚
â”‚         [CANCEL]  [SAVE]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 Enhanced Model Picker (Chat App Bar)

Current: flat list of all Ollama models  
New: grouped by endpoint, with search, showing only selected models

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–¸ SELECT MODEL                   â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ ğŸ” Search models...             â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â”€â”€ OLLAMA CLOUD â”€â”€               â”‚
â”‚ â—‹ qwen2.5:14b             9.0G  â”‚
â”‚ â— deepseek-r1:14b         9.0G  â”‚
â”‚ â—‹ gemma3:12b               8.1G  â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â”€â”€ OPENROUTER â”€â”€                 â”‚
â”‚ â—‹ anthropic/claude-3.5-sonnet    â”‚
â”‚ â—‹ openai/gpt-4o                  â”‚
â”‚ â—‹ google/gemini-2.0-flash        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Search behavior:** Filters across all endpoints simultaneously. Matching models remain visible under their endpoint headers (empty groups are hidden).

### 6.4 Backward Compatibility: Zero-Config Ollama

If the user has never configured endpoints but has the legacy `base_url` and `api_key` in SharedPreferences, we auto-create a default endpoint:

```dart
if (endpoints.isEmpty && legacyBaseUrl.isNotEmpty) {
  // Migrate: create a default "Ollama" endpoint from legacy settings
  createEndpoint(Endpoint(
    name: 'Ollama',
    baseUrl: legacyBaseUrl,
    apiKey: legacyApiKey,
    type: EndpointType.ollama,
    selectedModels: [], // empty initially â€” will be populated on first model fetch
  ));
}
```

**Model selection is required:** Every endpoint must have at least 1 model
selected. For a migrated legacy endpoint, we auto-select the previously-active
model. For new endpoints, the user must pick models before saving. This
prevents flooding the model picker with hundreds of models from providers
like OpenRouter.

### 6.5 App Bar Model Display

Current: `ExecPrompt â”‚ deepseek-r1:14b â–¾`  
Enhanced: `ExecPrompt â”‚ deepseek-r1:14b â–¾` (no change needed â€” model name is sufficient)

The endpoint name is visible in the grouped picker but doesn't need to clutter the app bar. The user knows which endpoint a model belongs to from the picker's section headers.

---

## 7. Web Search Tool Compatibility Analysis

### 7.1 Current Web Search Architecture

```
Tool flow:
1. User enables web search toggle
2. Chat request includes tools JSON
3. Ollama model decides to call web_search tool
4. ExecPrompt executes search (Ollama Cloud or Tavily)
5. Results injected as tool message
6. Model generates final response with citations
```

**Critical question:** Does web search continue to work when the chat model is NOT on Ollama?

### 7.2 Analysis: Web Search Independence

**The web search tool is endpoint-agnostic.** Here's why:

1. **Tool execution happens client-side.** ExecPrompt calls the Tavily or Ollama Cloud search API directly â€” this has nothing to do with which chat endpoint is active.

2. **The tool definition is standard.** Our `toOllamaToolJson()` already produces the OpenAI-standard `tools` format (it's the same JSON structure). We just need to rename the method to `toToolJson()` or keep it as-is.

3. **Tool results are injected as messages.** The tool result message (`role: "tool"`) is standard in both Ollama native and OpenAI spec.

**The one difference:** Ollama expects `tool_name` in the message, while OpenAI expects `tool_call_id`. Our adapter layer handles this translation.

### 7.3 Search Provider Strategy

| Chat Provider | Can use Ollama Cloud Search? | Can use Tavily? |
|---------------|------------------------------|-----------------|
| Ollama (any instance) | Yes (if API key configured) | Yes |
| OpenAI | Yes | Yes |
| OpenRouter | Yes | Yes |
| Any OpenAI-compatible | Yes | Yes |

**Result:** Web search works with ALL providers. The search service is completely independent of the chat service. Tavily is the especially robust option since it has no dependency on Ollama at all.

### 7.4 Tool Call Format Adaptation

For Ollama native API, our current tool call flow works as-is. For OpenAI-spec providers, we need to adapt:

**Ollama native â†’ current format:**
```json
// Response
{"message": {"tool_calls": [{"function": {"name": "web_search", "arguments": {"query": "..."}}}]}}
// Tool result
{"role": "tool", "content": "...", "tool_name": "web_search"}
```

**OpenAI spec â†’ new format:**
```json
// Response  
{"choices": [{"delta": {"tool_calls": [{"id": "call_123", "function": {"name": "web_search", "arguments": "{\"query\": \"...\"}"}}]}}]}
// Tool result
{"role": "tool", "content": "...", "tool_call_id": "call_123"}
```

The `OpenAiApiService` adapter handles this translation internally.

---

## 8. Future Tool Proposals

### 8.1 Tools to Consider (Not for Phase 8 implementation)

| Tool | Description | Feasibility | Value |
|------|-------------|-------------|-------|
| **Code Execution** | Run Python/JS snippets locally via an interpreter | MEDIUM â€” requires sandboxing | HIGH for dev users |
| **File System Read** | Read files from device storage | HIGH on desktop, MEDIUM on mobile | HIGH â€” enables context loading |
| **File System Write** | Write/save files to device storage | HIGH on desktop, MEDIUM on mobile | HIGH â€” enables code export |
| **URL Fetch** | Fetch and parse a specific URL | HIGH â€” simple HTTP | MEDIUM â€” complements web search |
| **Calculator** | Evaluate mathematical expressions | HIGH â€” pure Dart | LOW â€” models can do math |
| **Date/Time** | Get current date, time, timezone | HIGH â€” trivial | LOW â€” models hallucinate dates though |
| **Clipboard** | Read/write system clipboard | HIGH on desktop, MEDIUM on mobile | MEDIUM |
| **Image Generation** | Generate images via DALL-E / Stable Diffusion | MEDIUM â€” external API dependency | MEDIUM |

### 8.2 Recommended Next Tools (Post-Phase 8)

1. **URL Fetch / Web Page Reader** â€” Simple HTTP GET + HTML-to-text extraction. Extremely useful for reading documentation, articles, code files. Low implementation cost.

2. **File System Read (scoped)** â€” On mobile, use the `file_picker` plugin to let the user select files for the model to read. On desktop/Linux, allow reading from a configured workspace directory. This avoids permission issues while providing file access.

3. **File System Write (Downloads)** â€” Save generated content (code, configs, etc.) to the device's Downloads folder. On mobile, use `share_plus` (already in deps). On desktop, use `file_picker` save dialog.

4. **Date/Time Tool** â€” Trivial to implement. Returns current date, time, timezone, and relative date calculations. Helps models avoid date hallucination.

### 8.3 Tool Architecture Extensibility

Our current `ToolDefinition` abstract class is already well-designed for extensibility:
```dart
abstract class ToolDefinition {
  String get name;
  String get description;
  Map<String, dynamic> get parametersSchema;
  Map<String, dynamic> toOllamaToolJson();
  Future<String> execute(Map<String, dynamic> arguments);
}
```

Adding a new tool requires:
1. Create a new class extending `ToolDefinition`
2. Register it in `toolRegistryProvider`
3. Add a toggle in `ToolSettingsPanel`

No changes to chat flow needed â€” the tool dispatch loop in `ChatNotifier._streamAndHandleToolCalls()` is already generic.

---

## 9. Pre-Implementation Code Analysis

### 9.1 Files That Need Modification

| File | Change Type | Scope |
|------|-------------|-------|
| **`data/services/ollama_api_service.dart`** | Minor â€” add auto-detect method | Keep as-is, used for `ollama` type endpoints |
| **`data/models/chat_message.dart`** | Minor â€” add `toolCallId` field | For OpenAI-spec tool call ID tracking |
| **`data/models/chat_request.dart`** | Minor â€” no change needed (already has `tools`) | |
| **`data/models/chat_response.dart`** | No change | OpenAI responses handled by new service |
| **`data/models/ollama_model.dart`** | Minor â€” may add a `source` field or rename | |
| **`domain/providers/settings_provider.dart`** | Major â€” endpoints storage, migration | Core of the endpoint management |
| **`domain/providers/models_provider.dart`** | Major â€” multi-endpoint model aggregation | |
| **`domain/providers/chat_provider.dart`** | Medium â€” route requests through correct endpoint | |
| **`domain/providers/tool_provider.dart`** | Minor â€” tool JSON format, tool_call_id support | |
| **`domain/models/tool_definition.dart`** | Minor â€” rename `toOllamaToolJson` â†’ `toToolJson` | |
| **`presentation/screens/settings_screen.dart`** | Major â€” endpoint configuration UI | |
| **`presentation/screens/adaptive_shell.dart`** | Medium â€” enhanced model picker | |
| **`presentation/screens/chat_screen.dart`** | Medium â€” enhanced model picker (mirror) | |
| **`presentation/screens/models_screen.dart`** | Medium â€” multi-endpoint model management | |
| **`presentation/widgets/tool_settings_panel.dart`** | Minor â€” no structural change | |

### 9.2 New Files to Create

| File | Purpose |
|------|---------|
| **`data/services/api_adapter.dart`** | Abstract `ApiAdapter` base class |
| **`data/services/ollama_adapter.dart`** | Adapter wrapping existing `OllamaApiService` |
| **`data/services/openai_adapter.dart`** | OpenAI-compatible adapter (SSE streaming, /v1/ endpoints) |
| **`data/services/anthropic_adapter.dart`** | Anthropic Messages API adapter (SSE streaming, /v1/messages) |
| **`data/models/endpoint.dart`** | Endpoint data model with Freezed |
| **`data/models/openai_response.dart`** | OpenAI-format response models |
| **`data/models/model_info.dart`** | Unified model info (provider-agnostic) |
| **`domain/providers/endpoint_provider.dart`** | Endpoint CRUD, persistence, state management |
| **`presentation/screens/endpoint_config_screen.dart`** | Add/edit endpoint full-page screen |
| **`presentation/widgets/endpoint_list_section.dart`** | Settings section for endpoint management |
| **`presentation/widgets/model_picker.dart`** | Enhanced grouped/searchable model picker |

### 9.3 Migration Path

**Existing users** who have a `base_url` and `api_key` in SharedPreferences need transparent migration:

```dart
/// In EndpointsNotifier.init():
if (savedEndpoints.isEmpty) {
  final legacyUrl = prefs.getString('base_url');
  final legacyKey = prefs.getString('api_key');
  if (legacyUrl != null && legacyUrl.isNotEmpty) {
    final defaultEndpoint = Endpoint(
      id: uuid.v4(),
      name: 'Ollama',
      baseUrl: legacyUrl,
      apiKey: legacyKey,
      type: EndpointType.ollama,
      selectedModels: [],  // empty = show all
      sortOrder: 0,
      isActive: true,
      createdAt: DateTime.now(),
      updatedAt: DateTime.now(),
    );
    _saveEndpoint(defaultEndpoint);
  }
}
```

This ensures existing users see their current Ollama connection as a named endpoint with zero disruption.

---

## 10. Gap Analysis & Risk Assessment

### 10.1 Identified Gaps

| # | Gap | Severity | Mitigation |
|---|-----|----------|-----------|
| 1 | **SSE vs NDJSON streaming** â€” OpenAI uses SSE (`data: {...}`), Ollama uses NDJSON. Different parsing required. | HIGH | Implement dedicated SSE parser in `OpenAiApiService`. Well-understood format. |
| 2 | **Tool call ID tracking** â€” OpenAI requires `tool_call_id` to match results. Our current model uses `tool_name`. | MEDIUM | Add `toolCallId` to `ChatMessage`. OpenAI adapter generates/tracks IDs. Ollama path unchanged. |
| 3 | **Image format divergence** â€” Ollama: `images: ["base64"]`. OpenAI: `content: [{type: "image_url", ...}]`. | MEDIUM | Adapter in `OpenAiApiService` converts format. Internal model stays as-is. |
| 4 | **Massive model lists** (OpenRouter: 300+ models) â€” current bottom sheet will be unwieldy. | HIGH | Add search field to model picker. Mandatory for OpenRouter viability. |
| 5 | **Thinking content varies** â€” Different field names across providers. | LOW | Normalize in response adapter. Check `thinking`, `reasoning_content`, `thought` fields. |
| 6 | **Model metadata differences** â€” Ollama provides size/quant; OpenAI-spec provides minimal info. | LOW | Show available metadata per provider. Size column empty for OpenAI-spec models (or show context length instead). |
| 7 | **Options/parameters names** â€” Ollama: `options.top_k`, `num_predict`. OpenAI: `top_p`, `max_tokens`. | MEDIUM | Map names in request adapter. `top_k` not supported by most OpenAI-spec providers (skip it). |
| 8 | **Endpoint-specific features** â€” Ollama: `keep_alive`, model pulling. Not applicable to OpenAI-spec. | LOW | Conditionally show/hide features based on endpoint type. |
| 9 | **Token-based pricing** â€” OpenRouter/OpenAI responses include `usage` data. Currently ignored. | LOW | Future enhancement. Log but don't display for now. |
| 10 | **Concurrent model name collisions** â€” Same model name on different endpoints. | MEDIUM | Use `endpointId:modelName` as internal ID, display `modelName` with endpoint group header. |
| 11 | **Settings UI complexity** â€” Adding endpoint config without making Settings screen overwhelming. | MEDIUM | Dedicated endpoint config screen (navigated from Settings), not inline. Keeps Settings clean. |
| 12 | **Streaming argument accumulation** â€” OpenAI streams tool call arguments as string chunks. Ollama sends complete JSON. | HIGH | Accumulate argument string chunks, parse JSON only on `finish_reason: "tool_calls"`. |
| 13 | **Legacy settings migration** â€” Existing users need seamless transition. | HIGH | Auto-create default endpoint from legacy `base_url`/`api_key`. Transparent, zero disruption. |
| 14 | **flutter_secure_storage key management** â€” Multiple API keys per endpoint. | MEDIUM | Key naming: `endpoint_apikey_{endpoint_id}`. Migrate legacy `api_key` to default endpoint. |

### 10.2 Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|-----------|
| Breaking existing Ollama chat flow | LOW | CRITICAL | Feature branch. Ollama native path completely untouched. All changes additive. |
| SSE parsing edge cases | MEDIUM | HIGH | Thorough testing with real OpenRouter/OpenAI responses. Use established SSE parsing patterns. |
| Tool calls failing with OpenAI-spec | MEDIUM | MEDIUM | Web search tested against OpenRouter + Ollama. Tool dispatch loop is generic. |
| UI clutter / loss of terminal aesthetic | LOW | HIGH | All new UI follows existing CyberTerm patterns. Review each widget. |
| Performance degradation with many endpoints | LOW | LOW | Model lists cached in memory. Endpoint count expected < 10. |
| Data migration corruption | LOW | HIGH | Migration is additive â€” legacy keys preserved, new endpoint created alongside. |

### 10.3 Optimization Opportunities

1. **Model caching** â€” Cache fetched model lists per endpoint with TTL (e.g., 5 minutes). Avoid re-fetching on every model picker open.

2. **Lazy endpoint initialization** â€” Only create API service instances when actually used, not on app startup.

3. **Shared response model** â€” Use our existing `ChatResponse` for Ollama and a new `OpenAiChatResponse` for OpenAI-spec, with a shared `NormalizedResponse` that the chat provider consumes.

4. **Incremental UI delivery** â€” Ship endpoint configuration first, then enhanced model picker, then OpenAI service. Each step is independently testable.

### 10.4 What We're NOT Doing (Scope Boundaries)

- **No token usage tracking/display** â€” Future enhancement
- **No cost estimation** â€” Future enhancement  
- **No new tools** â€” Tool proposals documented but not implemented in Phase 8
- **No model pulling for non-Ollama** â€” Only Ollama supports model pulling
- **No multi-model parallel requests** â€” One model at a time
- **No conversation-level endpoint locking** â€” Models are hot-swappable even within a conversation

---

## 11. Implementation Plan

### 11.1 Development Phases

The implementation is broken into **6 sub-phases**, each independently testable and committable. This allows us to validate at each step and roll back if needed.

---

#### Phase 8A: Data Foundation â€” Endpoint Model & Storage
**Estimated effort:** 1-2 hours  
**Risk:** LOW  
**Regression risk:** ZERO (purely additive)

**Tasks:**
1. Create `data/models/endpoint.dart` â€” Endpoint data model with Freezed
2. Create `data/models/model_info.dart` â€” Provider-agnostic model info
3. Create `domain/providers/endpoint_provider.dart`:
   - `EndpointsNotifier` â€” CRUD operations
   - Persistence to SharedPreferences (JSON-encoded)
   - API key storage/retrieval via `flutter_secure_storage`
   - Legacy settings migration logic
4. Add `toolCallId` field to `ChatMessage`
5. Run `build_runner` to generate Freezed/JSON code

**Validation:**
- Unit test: create, save, load, delete endpoints
- Unit test: legacy migration from existing `base_url`/`api_key`
- Existing app behavior completely unchanged

---

#### Phase 8B: API Adapters â€” OpenAI & Anthropic
**Estimated effort:** 3-4 hours  
**Risk:** MEDIUM  
**Regression risk:** ZERO (new files, not integrated yet)

**Tasks:**
1. Create `data/services/api_adapter.dart` â€” abstract `ApiAdapter` class
2. Create `data/services/ollama_adapter.dart` â€” wraps existing `OllamaApiService`
3. Create `data/services/openai_adapter.dart`:
   - `streamChat(request)` â€” SSE streaming parser
   - `listModels()` â€” `/v1/models` endpoint
   - Request adapter: convert our `ChatRequest` to OpenAI format
   - Response adapter: convert OpenAI SSE chunks to our `ChatResponse` format
   - Tool call handling: accumulate streaming arguments, generate/track `tool_call_id`
   - Image format conversion: base64 â†’ `image_url` content blocks
   - Thinking normalization: check `reasoning_content`, `thinking` fields
   - OpenRouter-specific headers (`HTTP-Referer`, `X-Title`)
4. Create `data/services/anthropic_adapter.dart`:
   - `streamChat(request)` â€” SSE streaming for `/v1/messages`
   - `listModels()` â€” returns curated model list (no discovery endpoint)
   - System prompt extraction (top-level `system` field, not a message)
   - Tool use: `tool_use` content blocks â†’ `tool_result` content blocks
   - `anthropic-version` header, `x-api-key` header (instead of Bearer)
   - Thinking/extended thinking normalization
5. Create `data/models/openai_response.dart` â€” OpenAI-format response models (internal use only)

**Validation:**
- Manual test against OpenRouter API (requires API key)
- Manual test against OpenAI API (requires API key)
- Manual test against Anthropic API (if key available, else via OpenRouter)
- Manual test against Ollama's OpenAI-compatible endpoint (`/v1/chat/completions`)
- Verify streaming, tool calls, and thinking content across all adapters

---

#### Phase 8C: API Service Router & Chat Integration
**Estimated effort:** 2-3 hours  
**Risk:** MEDIUM-HIGH (touches chat flow)  
**Regression risk:** LOW (with careful design)

**Tasks:**
1. Create service router logic in endpoint provider:
   - Given a model name â†’ resolve to endpoint â†’ return correct service instance
   - Cache service instances per endpoint (don't recreate on every request)
2. Update `chat_provider.dart`:
   - Replace direct `ollamaApiServiceProvider` usage with endpoint-resolved adapter
   - Adapt tool call loop â€” the adapter's `buildToolResultMessage()` handles format differences
   - Handle `tool_call_id` for OpenAI/Anthropic tool results
3. Update `tool_provider.dart`:
   - Rename `toOllamaToolJson` â†’ `toToolJson` (or keep both)
   - Ensure tools JSON works with both API formats
4. Update `models_provider.dart`:
   - Multi-endpoint model aggregation
   - Per-endpoint model caching

**Critical design principle:** The Ollama native path (`OllamaApiService`) remains EXACTLY as it is, wrapped by `OllamaAdapter`. We add the OpenAI and Anthropic adapters alongside it. The router selects which adapter based on `EndpointType`.

**Validation:**
- Send message via Ollama endpoint â†’ works exactly as before
- Send message via OpenAI-compatible endpoint â†’ response streams correctly
- Web search tool works with both endpoint types
- Switch models between endpoints mid-conversation â†’ correct routing

---

#### Phase 8D: Settings UI â€” Endpoint Configuration
**Estimated effort:** 2-3 hours  
**Risk:** LOW  
**Regression risk:** ZERO (new screens, replaces connection section)

**Tasks:**
1. Create `presentation/widgets/endpoint_list_section.dart`:
   - List of configured endpoints in Settings
   - Shows name, URL, model count
   - Long-press or `[â‹®]` for edit/delete
   - `[+ ADD ENDPOINT]` button
2. Create `presentation/screens/endpoint_config_screen.dart`:
   - Full-screen form for add/edit endpoint
   - Name, type (Ollama/OpenAI), URL, API key fields
   - `[TEST CONNECTION]` button with status indicator
   - Scrollable model list with checkboxes
   - Search/filter for model list
   - Save/cancel actions
3. Update `presentation/screens/settings_screen.dart`:
   - Replace "CONNECTION" section with endpoint list section
   - Keep remaining sections (Theme, Model Config, Web Search, Data, Links) unchanged
4. Handle legacy migration UX (auto-migrate, don't prompt)

**Validation:**
- Add new endpoint â†’ connection test â†’ model list loads
- Select models â†’ save â†’ models persist
- Edit endpoint â†’ changes reflected
- Delete endpoint â†’ removed from model picker
- Existing Ollama connection auto-migrated on first launch

---

#### Phase 8E: Enhanced Model Picker
**Estimated effort:** 1-2 hours  
**Risk:** LOW  
**Regression risk:** LOW (replaces bottom sheet content, same trigger)

**Tasks:**
1. Create `presentation/widgets/model_picker.dart`:
   - Grouped by endpoint name
   - Search field at top
   - Endpoint section headers (styled: `â”€â”€ ENDPOINT NAME â”€â”€`)
   - Model rows with available metadata (size for Ollama, just name for OpenAI)
   - Current model highlighted
   - Empty state when no endpoints configured
2. Update `adaptive_shell.dart` â†’ `_showModelPicker()`:
   - Use new `ModelPicker` widget
   - Pass grouped model data
3. Update `chat_screen.dart` â†’ `_showModelPicker()`:
   - Same update (this screen is a fallback for the old layout)
4. Update `models_screen.dart`:
   - Show models grouped by endpoint
   - Conditionally show pull/delete actions (Ollama-only features)

**Validation:**
- Model picker shows grouped models from multiple endpoints
- Search filter works across all groups
- Selecting a model from any endpoint works
- Empty endpoint groups hidden when search active
- Single Ollama endpoint looks identical to current experience

---

#### Phase 8F: Polish, Testing & Documentation
**Estimated effort:** 1-2 hours  
**Risk:** LOW  

**Tasks:**
1. End-to-end testing:
   - Fresh install experience (no endpoints â†’ guided setup)
   - Legacy migration (existing base_url â†’ auto-endpoint)
   - Multi-endpoint workflow (Ollama + OpenRouter)
   - Web search with non-Ollama endpoint
   - Image attachment with non-Ollama endpoint
   - Thinking content from different providers
2. UI polish:
   - Consistent CyberTerm styling on all new elements
   - Loading states for model list fetching
   - Error states for failed connections
   - Haptic feedback on all interactions
3. Detail panel updates:
   - Show endpoint name in MODEL INFO section
   - Show endpoint type indicator
4. Documentation:
   - Update `DEVELOPMENT.md` with multi-provider architecture
   - Update `PROJECT_SUMMARY.md`
5. Commit and verify build

---

### 11.2 Implementation Order & Dependencies

```
Phase 8A â”€â”€â”€â”€â”€â”
              â”‚
Phase 8B â”€â”€â”€â”€â”€â”¤â”€â”€â†’ Phase 8C â”€â”€â†’ Phase 8D â”€â”€â†’ Phase 8E â”€â”€â†’ Phase 8F
              â”‚
(parallel)    â”‚
```

- **8A and 8B are independent** â€” can be developed in parallel
- **8C depends on 8A + 8B** â€” integrates both
- **8D depends on 8A** (endpoint model) but not 8B/8C
- **8E depends on 8A** (endpoint model) and ideally 8C (for real model data)
- **8F depends on all previous**

**Optimal order:** 8A â†’ 8B â†’ 8C â†’ 8D â†’ 8E â†’ 8F

### 11.3 Commit Strategy

Each phase gets its own commit with a clear message:
```
8A: feat: endpoint data model and storage layer
8B: feat: pluggable API adapters (OpenAI + Anthropic)
8C: feat: API service router and chat integration
8D: feat: endpoint configuration UI
8E: feat: enhanced grouped model picker with search
8F: chore: polish, testing, documentation
```

### 11.4 Rollback Strategy

The feature branch `feature/multi-provider-endpoints` protects `main`. If we encounter critical issues:
1. Each sub-phase commit is a recovery point
2. The entire branch can be abandoned without affecting `main`
3. The legacy settings migration is non-destructive (original keys preserved)

---

## Summary

This Phase 8 enhancement transforms ExecPrompt from a single-provider Ollama client into a multi-provider AI command center. The implementation is carefully designed to:

- **Protect the existing experience** through additive-only changes and a feature branch
- **Handle all major providers** through a pluggable adapter architecture (Ollama native + OpenAI-compatible + Anthropic)
- **Maintain web search functionality** across all providers (search service is endpoint-agnostic)
- **Scale gracefully** from 1 endpoint to many, with search and model curation to manage large catalogs
- **Stay on brand** with CyberTerm terminal aesthetic throughout

The 6-phase implementation plan delivers incrementally testable milestones, with the full feature complete in an estimated 10-16 hours of development time.
